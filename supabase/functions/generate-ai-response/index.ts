import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const SYSTEM_PROMPT = `Voc√™ √© um assistente especializado em analisar conversas e gerar respostas que IMITAM PERFEITAMENTE o estilo de comunica√ß√£o do atendente.

## SUA MISS√ÉO
Analisar a conversa COMPLETA e gerar uma resposta que:
1. Pare√ßa ter sido escrita pelo pr√≥prio atendente (n√£o por uma IA)
2. Use exatamente o mesmo tom, vocabul√°rio e estilo observado
3. Seja contextualmente perfeita para a situa√ß√£o
4. Ajude o atendente quando ele n√£o souber o que responder

## AN√ÅLISE OBRIGAT√ìRIA (fa√ßa antes de responder)

### Passo 1: Identificar o padr√£o do ATENDENTE
Analise TODAS as mensagens marcadas como [ATENDENTE] e identifique:
- N√≠vel de formalidade (formal/semiformal/informal/muito informal)
- Uso de emojis (usa muito/pouco/nunca? quais tipos?)
- Cumprimentos t√≠picos (como ele sa√∫da? "Oi", "Ol√°", "E a√≠"?, "Bom dia"?)
- Despedidas t√≠picas (como ele finaliza?)
- Comprimento das respostas (curtas e diretas ou elaboradas?)
- Express√µes recorrentes (g√≠rias, bord√µes, frases caracter√≠sticas)
- Pontua√ß√£o (usa muitas exclama√ß√µes? retic√™ncias? ponto final?)
- Capitaliza√ß√£o (tudo min√∫sculo? normal? CAPS para √™nfase?)
- Uso de abrevia√ß√µes (vc, pq, tb, etc. ou por extenso?)

### Passo 2: Entender o CONTEXTO da conversa
- Qual √© o assunto principal sendo discutido?
- Qual √© a √∫ltima pergunta/solicita√ß√£o do cliente?
- H√° algum problema a ser resolvido?
- Qual √© o estado emocional do cliente? (satisfeito, irritado, confuso, ansioso?)
- O que o cliente espera como resposta?
- H√° informa√ß√µes anteriores na conversa que podem ajudar?

### Passo 3: Gerar resposta IMITANDO o atendente
Use EXATAMENTE o mesmo padr√£o identificado no Passo 1.

## REGRAS CR√çTICAS

1. IMITE o atendente - se ele usa "vc", use "vc"; se ele usa "voc√™", use "voc√™"
2. COPIE o n√≠vel de emoji - se ele usa üòä frequentemente, use tamb√©m; se n√£o usa, n√£o use
3. MANTENHA o comprimento t√≠pico - se ele √© conciso, seja conciso; se √© detalhado, seja detalhado
4. PRESERVE o vocabul√°rio - use as mesmas palavras e express√µes dele
5. RESPONDA apenas ao que foi perguntado - seja relevante e direto
6. N√ÉO invente informa√ß√µes que n√£o existem na conversa
7. N√ÉO use formata√ß√£o markdown (sem asteriscos, bullets, hashtags, etc.)
8. N√ÉO inclua sauda√ß√µes se a conversa j√° est√° em andamento (meio da conversa)
9. SE n√£o houver mensagens suficientes do atendente, use tom profissional neutro mas amig√°vel
10. SE o cliente est√° frustrado ou irritado, seja emp√°tico e compreensivo
11. Use o nome do cliente quando apropriado para personaliza√ß√£o
12. BASEIE sua resposta 100% no contexto da conversa
13. SE houver imagens enviadas pelo cliente, ANALISE o conte√∫do visual e responda considerando o que est√° na imagem

## FORMATO DA CONVERSA
- [CLIENTE]: mensagens enviadas pelo cliente
- [ATENDENTE]: mensagens enviadas pelo atendente (ESTUDE ESTE PADR√ÉO!)

## EXEMPLOS DE IMITA√á√ÉO

### Exemplo 1 - Atendente informal
Se o atendente escreveu:
"[ATENDENTE]: oi! td bem? em q posso te ajudar hj? üòä"
"[ATENDENTE]: achei aqui! ta previsto pra amanh√£"

Sua resposta deve seguir o mesmo padr√£o:
"entendi! vou verificar isso pra vc agora, s√≥ um momento üòä"

E N√ÉO:
"Entendi. Vou verificar isso para voc√™ agora. Aguarde um momento."

### Exemplo 2 - Atendente formal
Se o atendente escreveu:
"[ATENDENTE]: Bom dia! Como posso ajud√°-lo hoje?"
"[ATENDENTE]: Perfeito, vou verificar essa informa√ß√£o para voc√™."

Sua resposta deve seguir o mesmo padr√£o:
"Compreendo sua preocupa√ß√£o. Vou analisar a situa√ß√£o e retornar com uma solu√ß√£o."

### Exemplo 3 - Atendente direto sem emoji
Se o atendente escreveu:
"[ATENDENTE]: oi"
"[ATENDENTE]: ok, vou ver"

Sua resposta deve ser igualmente direta:
"entendi, vou resolver"

## INFORMA√á√ïES ADICIONAIS DISPON√çVEIS
- Nome do atendente: pode ser usado internamente para contexto
- Departamento: indica a √°rea de atua√ß√£o (vendas, suporte, etc.)
- Tags da conversa: indicam o assunto/categoria

Retorne APENAS a resposta final, sem an√°lises, explica√ß√µes ou coment√°rios.`;

interface MessageInput {
  content: string | null;
  direction: 'inbound' | 'outbound';
  messageType: string;
  mediaUrl?: string;
  metadata?: {
    transcription?: string;
    fileName?: string;
    file_name?: string;
  };
}

interface RequestBody {
  messages: MessageInput[];
  contactName: string;
  agentName?: string;
  department?: string;
  tags?: string[];
}

// Helper function to convert ArrayBuffer to base64
function arrayBufferToBase64(buffer: ArrayBuffer): string {
  const bytes = new Uint8Array(buffer);
  let binary = '';
  for (let i = 0; i < bytes.byteLength; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
}

// Helper function to transcribe audio using Gemini
async function transcribeAudio(audioUrl: string, apiKey: string): Promise<string | null> {
  try {
    console.log('üé§ Transcrevendo √°udio com Gemini:', audioUrl.substring(0, 80) + '...');
    
    const audioResponse = await fetch(audioUrl);
    if (!audioResponse.ok) {
      console.log('‚ùå Falha ao baixar √°udio:', audioResponse.status);
      return null;
    }
    
    const audioBuffer = await audioResponse.arrayBuffer();
    const base64Audio = arrayBufferToBase64(audioBuffer);
    const contentType = audioResponse.headers.get('content-type') || 'audio/ogg';
    
    // Determine mime type
    let mimeType = 'audio/ogg';
    if (contentType.includes('mp3') || contentType.includes('mpeg')) mimeType = 'audio/mp3';
    else if (contentType.includes('wav')) mimeType = 'audio/wav';
    else if (contentType.includes('webm')) mimeType = 'audio/webm';
    else if (contentType.includes('m4a')) mimeType = 'audio/mp4';
    else if (contentType.includes('ogg')) mimeType = 'audio/ogg';
    
    const geminiResponse = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{
            parts: [
              { text: 'Transcreva este √°udio em portugu√™s brasileiro. Retorne APENAS o texto transcrito, sem explica√ß√µes ou coment√°rios adicionais.' },
              {
                inline_data: {
                  mime_type: mimeType,
                  data: base64Audio
                }
              }
            ]
          }],
          generationConfig: {
            temperature: 0.1,
            maxOutputTokens: 4000
          }
        }),
      }
    );
    
    if (!geminiResponse.ok) {
      const errorText = await geminiResponse.text();
      console.log('‚ùå Erro Gemini transcri√ß√£o:', geminiResponse.status, errorText);
      return null;
    }
    
    const result = await geminiResponse.json();
    const transcription = result.candidates?.[0]?.content?.parts?.[0]?.text?.trim();
    
    if (transcription) {
      console.log('‚úÖ √Åudio transcrito:', transcription.substring(0, 50) + '...');
      return transcription;
    }
    
    return null;
  } catch (error) {
    console.error('‚ùå Erro ao transcrever √°udio:', error);
    return null;
  }
}

// Helper function to fetch image and convert to base64
async function fetchImageAsBase64(imageUrl: string): Promise<{ data: string; mimeType: string } | null> {
  try {
    console.log('üñºÔ∏è Baixando imagem:', imageUrl.substring(0, 80) + '...');
    
    const response = await fetch(imageUrl);
    if (!response.ok) {
      console.log('‚ùå Falha ao baixar imagem:', response.status);
      return null;
    }
    
    const buffer = await response.arrayBuffer();
    const base64 = arrayBufferToBase64(buffer);
    const contentType = response.headers.get('content-type') || 'image/jpeg';
    
    // Normalize mime type
    let mimeType = 'image/jpeg';
    if (contentType.includes('png')) mimeType = 'image/png';
    else if (contentType.includes('gif')) mimeType = 'image/gif';
    else if (contentType.includes('webp')) mimeType = 'image/webp';
    
    console.log('‚úÖ Imagem convertida para base64, tipo:', mimeType);
    return { data: base64, mimeType };
  } catch (error) {
    console.error('‚ùå Erro ao baixar imagem:', error);
    return null;
  }
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { messages, contactName, agentName, department, tags } = await req.json() as RequestBody;

    if (!messages || messages.length === 0) {
      return new Response(
        JSON.stringify({ error: 'Nenhuma mensagem fornecida' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const geminiApiKey = Deno.env.get('GEMINI_API_KEY');
    if (!geminiApiKey) {
      console.error('‚ùå GEMINI_API_KEY not configured');
      return new Response(
        JSON.stringify({ error: 'API key not configured' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // Limit to last 100 messages
    const recentMessages = messages.slice(-100);
    
    // Collect images for multimodal analysis
    const imageUrls: string[] = [];
    
    // Process messages and collect media
    const processedMessages: string[] = [];
    
    for (const msg of recentMessages) {
      const prefix = msg.direction === 'inbound' ? '[CLIENTE]' : '[ATENDENTE]';
      let content = msg.content;
      const metadata = msg.metadata;
      
      if (msg.messageType === 'audio') {
        // Handle audio - use transcription if available, otherwise transcribe
        if (metadata?.transcription) {
          content = `[√Åudio transcrito]: ${metadata.transcription}`;
        } else if (msg.mediaUrl) {
          const transcription = await transcribeAudio(msg.mediaUrl, geminiApiKey);
          content = transcription 
            ? `[√Åudio transcrito]: ${transcription}`
            : '[√Åudio sem transcri√ß√£o dispon√≠vel]';
        } else {
          content = '[Mensagem de √°udio]';
        }
      } else if (msg.messageType === 'image') {
        // Collect image URL for multimodal analysis
        if (msg.mediaUrl) {
          imageUrls.push(msg.mediaUrl);
          content = msg.content 
            ? `[Imagem com legenda: ${msg.content}] (imagem ser√° analisada)`
            : '[Cliente enviou uma imagem] (imagem ser√° analisada)';
        } else {
          content = msg.content 
            ? `[Imagem com legenda]: ${msg.content}`
            : '[Cliente enviou uma imagem]';
        }
      } else if (msg.messageType === 'video') {
        content = msg.content 
          ? `[V√≠deo com legenda]: ${msg.content}`
          : '[Cliente enviou um v√≠deo]';
      } else if (msg.messageType === 'document') {
        const fileName = metadata?.fileName || metadata?.file_name || 'documento';
        content = msg.content 
          ? `[Documento "${fileName}"]: ${msg.content}`
          : `[Cliente enviou documento: ${fileName}]`;
      } else if (msg.messageType === 'sticker') {
        content = '[Cliente enviou um sticker]';
      } else if (!content || content.trim() === '') {
        content = '[Mensagem sem texto]';
      }
      
      processedMessages.push(`${prefix}: ${content}`);
    }
    
    const formattedMessages = processedMessages.join('\n');

    // Build enriched system prompt
    let enrichedSystemPrompt = SYSTEM_PROMPT;
    const contextParts: string[] = [];
    
    if (contactName && contactName !== 'Cliente') {
      contextParts.push(`Nome do cliente: ${contactName}`);
    }
    if (agentName) {
      contextParts.push(`Nome do atendente: ${agentName}`);
    }
    if (department) {
      contextParts.push(`Departamento: ${department}`);
    }
    if (tags && tags.length > 0) {
      contextParts.push(`Tags da conversa: ${tags.join(', ')}`);
    }
    
    if (contextParts.length > 0) {
      enrichedSystemPrompt += `\n\n## CONTEXTO DESTA CONVERSA\n${contextParts.join('\n')}`;
    }

    console.log('ü§ñ Gerando resposta com Gemini para', contactName);
    console.log('üìä Total de mensagens:', recentMessages.length);
    console.log('üñºÔ∏è Imagens para analisar:', imageUrls.length);
    console.log('üë§ Atendente:', agentName || 'N/A');

    // Build the prompt
    const fullPrompt = `${enrichedSystemPrompt}\n\nAnalise esta conversa e gere a pr√≥xima resposta imitando o estilo do atendente:\n\n${formattedMessages}`;

    // Build parts array for Gemini
    const parts: any[] = [{ text: fullPrompt }];

    // If there are images, fetch and add them as inline_data
    if (imageUrls.length > 0) {
      console.log('üñºÔ∏è Processando imagens para an√°lise multimodal...');
      
      // Limit to last 5 images to avoid token limits
      const imagesToAnalyze = imageUrls.slice(-5);
      
      for (const imageUrl of imagesToAnalyze) {
        const imageData = await fetchImageAsBase64(imageUrl);
        if (imageData) {
          parts.push({
            inline_data: {
              mime_type: imageData.mimeType,
              data: imageData.data
            }
          });
        }
      }
      
      console.log('‚úÖ Imagens processadas:', parts.length - 1);
    }

    // Call Gemini API
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${geminiApiKey}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts }],
          generationConfig: {
            temperature: 0.4,
            maxOutputTokens: 4000
          }
        }),
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error('‚ùå Gemini API error:', response.status, errorText);
      return new Response(
        JSON.stringify({ error: 'Erro ao gerar resposta' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const data = await response.json();
    console.log('üì¶ Gemini response received');
    
    const generatedResponse = data.candidates?.[0]?.content?.parts?.[0]?.text?.trim();

    if (!generatedResponse) {
      console.error('‚ùå No content in response:', JSON.stringify(data, null, 2));
      return new Response(
        JSON.stringify({ error: 'Nenhuma resposta gerada' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    console.log('‚úÖ Resposta gerada com sucesso');
    console.log('üìù Resposta:', generatedResponse.substring(0, 100) + '...');

    return new Response(
      JSON.stringify({ response: generatedResponse }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  } catch (error) {
    console.error('‚ùå Error in generate-ai-response function:', error);
    return new Response(
      JSON.stringify({ error: error instanceof Error ? error.message : 'Unknown error' }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
